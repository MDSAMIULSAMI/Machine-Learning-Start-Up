{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8737802,"sourceType":"datasetVersion","datasetId":5245645}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![Sami](https://scontent.fdac178-1.fna.fbcdn.net/v/t39.30808-6/398173458_212185451895611_2740129820745384762_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=5f2048&_nc_eui2=AeG-JZ_D-TuX1tPLp6RRtYv76bBYGzSjgNvpsFgbNKOA27wm9UCdQHpc_FWroK0-3PD3u1xM032yK-MjdjjqAsRs&_nc_ohc=chEyTwOPCnkQ7kNvgF-r4ce&_nc_ht=scontent.fdac178-1.fna&oh=00_AYD8r4XjEk4rJKIpff6jbZvgJWCqsUhDhIJg8QkVzcaiaQ&oe=6679E45D)","metadata":{}},{"cell_type":"markdown","source":"**This is tesing Image for captioning**","metadata":{}},{"cell_type":"code","source":"image_url = 'https://scontent.fdac178-1.fna.fbcdn.net/v/t39.30808-6/398173458_212185451895611_2740129820745384762_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=5f2048&_nc_eui2=AeG-JZ_D-TuX1tPLp6RRtYv76bBYGzSjgNvpsFgbNKOA27wm9UCdQHpc_FWroK0-3PD3u1xM032yK-MjdjjqAsRs&_nc_ohc=chEyTwOPCnkQ7kNvgF-r4ce&_nc_ht=scontent.fdac178-1.fna&oh=00_AYD8r4XjEk4rJKIpff6jbZvgJWCqsUhDhIJg8QkVzcaiaQ&oe=6679E45D'","metadata":{"execution":{"iopub.status.busy":"2024-06-20T13:25:18.168925Z","iopub.execute_input":"2024-06-20T13:25:18.169639Z","iopub.status.idle":"2024-06-20T13:25:18.181000Z","shell.execute_reply.started":"2024-06-20T13:25:18.169591Z","shell.execute_reply":"2024-06-20T13:25:18.179925Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# ***GPT-2 Image Captioning***","metadata":{}},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\nfeature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ntokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n\ndef predict_step(image_sources):\n    images = []\n    for image_source in image_sources:\n        if image_source.startswith('http://') or image_source.startswith('https://'):\n            response = requests.get(image_source)\n            i_image = Image.open(BytesIO(response.content))\n        else:\n            i_image = Image.open(image_source)\n        \n        if i_image.mode != \"RGB\":\n            i_image = i_image.convert(mode=\"RGB\")\n\n        images.append(i_image)\n\n    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n    pixel_values = pixel_values.to(device)\n\n    output_ids = model.generate(pixel_values, **gen_kwargs)\n\n    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n    preds = [pred.strip() for pred in preds]\n    return preds\n\npredictions = predict_step([image_url])\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T13:25:18.186991Z","iopub.execute_input":"2024-06-20T13:25:18.187299Z","iopub.status.idle":"2024-06-20T13:25:36.857680Z","shell.execute_reply.started":"2024-06-20T13:25:18.187264Z","shell.execute_reply":"2024-06-20T13:25:36.856374Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-20 13:25:22.309465: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-20 13:25:22.309529: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-20 13:25:22.311150: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\nYou may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n","output_type":"stream"},{"name":"stdout","text":"['a man in a suit and tie standing in a field']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ***BLIP Image Captioning (Base)***","metadata":{}},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n\nimg_url = image_url\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(\"Caption:\",processor.decode(out[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-06-20T13:25:36.859596Z","iopub.execute_input":"2024-06-20T13:25:36.860426Z","iopub.status.idle":"2024-06-20T13:25:40.343986Z","shell.execute_reply.started":"2024-06-20T13:25:36.860382Z","shell.execute_reply":"2024-06-20T13:25:40.342875Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Caption: a man standing in front of a tree\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ***BLIP Image Captioning (Large)***","metadata":{}},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")\n\nimg_url = image_url\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(\"Caption:\",processor.decode(out[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-06-20T13:25:40.346878Z","iopub.execute_input":"2024-06-20T13:25:40.347314Z","iopub.status.idle":"2024-06-20T13:25:45.116971Z","shell.execute_reply.started":"2024-06-20T13:25:40.347264Z","shell.execute_reply":"2024-06-20T13:25:45.115757Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Caption: there is a man standing in a park with a neck tie\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ***An Image Describer - XTuner: A Toolkit for Efficiently Fine-tuning LLM***","metadata":{}},{"cell_type":"code","source":"import requests\nfrom PIL import Image\n\nimport torch\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n\nmodel_id = \"xtuner/llava-phi-3-mini-hf\"\nprompt = \"<|user|>\\n<image>\\nWhat are these?<|end|>\\n<|assistant|>\\n\"\nimage_file = image_url\n\ntorch.cuda.empty_cache()\ntorch.backends.cuda.max_split_size_mb = 32\n\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=True, \n    device_map=\"auto\"\n)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nresponse = requests.get(image_url, stream=True)\nraw_image = Image.open(response.raw)\n\ninputs = processor(prompt, raw_image, return_tensors='pt').to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), torch.float16)\n\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-06-20T13:25:45.118528Z","iopub.execute_input":"2024-06-20T13:25:45.118983Z","iopub.status.idle":"2024-06-20T13:25:57.308915Z","shell.execute_reply.started":"2024-06-20T13:25:45.118944Z","shell.execute_reply":"2024-06-20T13:25:57.307798Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c35c364aa7e420a87ce367c56b83398"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"\nWhat are these? The image features a man standing in a park, surrounded by trees. He is wearing a blue shirt, glasses, and a lanyard around his neck. The lanyard has a badge attached to it, which is visible in the image. The man appears to be enjoying his time in the park, possibly attending an event or gathering. The trees in the background provide a natural and serene setting for the scene.\n","output_type":"stream"}]}]}